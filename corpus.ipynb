{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jsonlines\n",
    "from process_data import DataProcessor\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"./example-data/comments\"\n",
    "OUTPUT_PATH = \"./example-output\"\n",
    "MODE = \"comments\"\n",
    "MAX_LINES = 20000\n",
    "TOKENS = 50\n",
    "KEYWORDS = [\"community\", \"subreddit\", \"identity\", \"belonging\", \"group\", \"people\"]\n",
    "REGEX = [r\"[Cc]ommunity|[Cc]ommunities\", r\"[Ss]ubreddits?\", r\"[Ii]dentity|[Ii]dentities\", r\"[Bb]elonging\", r\"[Gg]roups?\", r\"[Pp]eople\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check for keywords\n",
    "\n",
    "def check_keywords(regex:list, text: str):\n",
    "    regex_list = []\n",
    "    for i in regex:\n",
    "        r = re.compile(i)\n",
    "        regex_list.append(r)\n",
    "    for x in regex_list:\n",
    "        if x.search(text):\n",
    "            return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = DataProcessor(\n",
    "        input_path=INPUT_PATH,\n",
    "        output_directory=OUTPUT_PATH,\n",
    "        mode=MODE,\n",
    "    )\n",
    "gen = p.get_generator()\n",
    "count = 0\n",
    "name = 1\n",
    "output_path = Path(p.output_directory) / f\"test_corpus_{name}.txt\"\n",
    "\n",
    "for line in gen:\n",
    "    with output_path.open(mode=\"a+\", encoding=\"utf-8\") as f:\n",
    "        text = line.get(\"body\")\n",
    "        if text:\n",
    "            if line.get(\"author\") == \"AutoModerator\":\n",
    "                pass\n",
    "            else:\n",
    "                l = len(wordpunct_tokenize(text))\n",
    "                if l >= TOKENS:\n",
    "                    comment = check_keywords(regex=REGEX, text=text)\n",
    "                    if comment is not None:\n",
    "                        count += 1\n",
    "                        f.write(comment + \"\\n\")\n",
    "                else:\n",
    "                    pass\n",
    "        if count >= MAX_LINES:\n",
    "            count = 0\n",
    "            name += 1\n",
    "            output_path = Path(p.output_directory) / f\"test_corpus_{name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5133d6627b29444d67e912e327b4a13b7465446dc6d927674a36a5be79b6f7b5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pipje')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
